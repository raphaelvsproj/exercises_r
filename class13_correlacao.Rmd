---
title: "Correlação Linear de Pearson no R (trabalhando com dados em painel)"
output: 
  html_document:
    highlight: textmate
    includes:
      in_header: "cabecalho.html"
    theme: flatly
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---
<p style="text-align: justify;">Trabalharemos com correlação bivariada (entre duas variáveis), e utilizaremos três tipos de correlação linear: a correlação de *Pearson* (paramétrica, e deverá atender alguns pressupostos para ser usada), a correlação não-paramétrica de *Spearman* e a também não-paramétrica de *Kendall*.</p>  
  
# O que vamos executar?  
  
* Verificar os pressupostos para a correlação de *Pearson* (normalidade, homocedasticidade, relação linear e ausência de *outliers*);  
* Fazer os testes de correlação de *Pearson*(paramétrico) e as correlações não-paramétricas de *Spearman* e *Kendall*;  
* Pedir o gráfico de dispersão e a matriz de correlação entre as variáveis.  
  
<br>
<br>
  
# Situação problema  
  
<p style="text-align: justify;">O `Banco de Dados 10.csv` contém informações de 103 alunos submetidos a uma prova. Verifique se há relação entre a ansiedade pré-prova e o desempenho dos alunos. Descreva os resultados de forma apropriada.</p>  
  
<br>
<br>
  
# Efetuando a Análise  
  
É necessário seguir alguns passos importantes.  
  
## Passo 1: Carregar os pacotes de dados.  
  
```{r, warning=FALSE, message=FALSE}
if(!require(dplyr)) install.packages("dplyr")
library(dplyr)                                
if(!require(car)) install.packages("corrplot")   
library(corrplot)                                
if(!require(ggplot2)) install.packages("ggplot2") 
library(ggplot2)
```
  
<br>
  
## Passo 2: Carregar o banco de dados.  
  
<p style="text-align: justify;">Detalhe importante: selecionar o diretório de trabalho via *Session/Set Working Directory/Choose Directory*.</p>  
  
```{r, warning=FALSE, message=FALSE}
dados <- read.csv2('Banco de Dados 10.csv', encoding = 'latin1')
glimpse(dados)
library(rmarkdown)
paged_table(dados)
```
  
<br>
  
## Passo 3: Verificação dos pressupostos para a correlação de *Pearson*  
  
<p style="text-align: justify;">Lembrando que a correlação de *Pearson* só vai fazer sentido dentro de um determinado contexto, ou seja, tendo atendido os pressupostos de normalidade, homogeneidade e verificação de *outliers*.</p>    
  
### Verificação da normalidade - `shapiro.test`:  
  
```{r, warning=FALSE, message=FALSE}
shapiro.test(dados$Ansiedade)
shapiro.test(dados$Nota)
```
  
<p style="text-align: justify;">Pressupostos:</p>  
* H~0~: Distribuição dos dados = normal → p > 0,05  
* H~1~: Distribuição dos dados ≠ normal → p ≤ 0,05  
  
<p style="text-align: justify;">Verificamos assim que nem `Ansiedade` e nem `Nota` são variáveis que possuem distribuição normal, pois em ambos os casos, `p < 0.05`.</p>  
  
<br>
  
### Verificação da presença de *outliers*:  
  
<p style="text-align: justify;">Podemos usar o próprio gráfico de `boxplot`, ou a função `identify_outliers` do pacote `rstatix`, lembrando que ambos acabam chegando a conclusões diferentes porque, por mais que utilizem o mesmo critério (baseando o *outlier* como um valor fora do 1,5 da amplitude interquartil), a grande diferença é que o `boxplot` usa os quartis calculados excluindo a mediana, e a função `identify_outliers` inclui a mediana. Em conjuntos muito grande de dados, isso não faz diferença, mas em conjunto menor de dados (como o caso desse exercício), verificamos diferença.</p>

```{r, warning=FALSE, message=FALSE}
boxplot(dados$Ansiedade)
boxplot(dados$Nota)
```
  
<p style="text-align: justify;">Pelo gráfico podemos ver que existem *outliers* para a variável `Ansiedade`, mas não para `Nota`. No entanto, outra coisa importante para ser verificado, antes de fazer a correlação de *Pearson*, é entender como a correlação entre as variáveis se comporta. Quando aplicamos a correlação de *Pearson*, ela verifica a correlação linear entre as variáveis. Olhando para o padrão de distribuição dos dados em um gráfico, é possível ver se, caso não exista uma correlação linear, pode ser que haja uma relação quadrática (se os dados estiverem formando os pontos em uma parábola, por exemplo).</p>     
  
```{r, warning=FALSE, message=FALSE}
plot(dados$Ansiedade, dados$Nota)
```
  
<p style="text-align: justify;">Nesse caso, vemos que a nuvem de pontos assume um comportamento ainda de difícil comparação com uma reta, ou mesmo com uma parábora.</p>  
  
<br>
  
## Passo 4: Verificação dos pressupostos nos resíduos  
  
<p style="text-align: justify;">Outro pressuposto muito importante é a homocedasticidade, e para testar precisamos construir primeiro um modelo de regressão linear porque conseguimos visualizar melhor a homocedasticidade quando olhamos para os resíduos, e não para os dados brutos.</p>  
  
### Construção do modelo:  
  
```{r, warning=FALSE, message=FALSE}
mod_reg <- lm(Nota ~ Ansiedade, dados)
```
  
<br>
  
### Análise gráfica:  
  
```{r, warning=FALSE, message=FALSE}
par(mfrow=c(1,2))
plot(mod_reg, which=c(1,3))
par(mfrow=c(1,1))
```

<p style="text-align: justify;">Inserimos, ao final do código `plot(mod_reg, which=c(1,3))` dois gráficos padronizados do R, no comando plot, sendo:</p>  
  
Gráfico 1: valores previstos x resíduos  

<p style="text-align: justify;">Permite verificar se há homogeneidade de variâncias (homocedasticidade) - resíduos distribuídos de acordo com um padrão aproximadamente retangular. Também permite verificar se a relação entre as variáveis é linear, com a linha vermelha aproximadamente horizontal.</p>  
  
Gráfico 3: valores previstos x resíduos padronizados  
<p style="text-align: justify;">Permite verificar se há outliers - valores de resíduos padronizados acima de 3 ou abaixo de -3.</p>  
<p style="text-align: justify;">Dessa forma, analisando os dois gráficos, podemos ver que não existe homocedasticidade porque os erros não estão padronizadamente distribuídos - verifica-se uma diminuição na diferença dos valores esperados e dos valores apresentados a medida que aumentamos o valor da amostra, ou seja, o modelo é **heterocedástico** (não existe homocedasticidade)</p>
  
<p style="text-align: justify;">Dado o exposto, verificamos que o modelo não atende aos pressupostos da correlação de *Pearson*, que seria a normalidade dos dados, a verificação de *outliers* e a homocedasticidade do modelo, mas faremos a correlação mesmo assim a fim de estudarmos como o modelo se comporta.</p>  
  
<br>
  
## Passo 5: Realização da correlação  
  
### Correlação *Linear de Pearson* (coeficiente = r):  
  
<p style="text-align: justify;">A fim de entendermos a ideia por trás do teste, precisamos ver a hipótese nula e a hipótese alternativa, conforme a regra:  
  
H~0~: Coeficiente = 0 → p > 0,05  
H~1~: Coeficiente ≠ 0 → p ≤ 0,05  
  
<p style="text-align: justify;">Verificamos que o valor de `p < 0`, apesar de o valor apresentado de correlação ser fraco, estatisticamente é diferente de zero.</p>  
  
```{r, warning=FALSE, message=FALSE}
cor.test(dados$Nota, dados$Ansiedade, method = "pearson")
```
  
<br>
  
### Correlação de *Postos de Spearman* (coeficiente = r):  
  
```{r, warning=FALSE, message=FALSE}
cor.test(dados$Nota, dados$Ansiedade, method = "spearman")
```
  
<br>
  
### Correlação *Tau de Kendall* (coeficiente = tau):

```{r, warning=FALSE, message=FALSE}
cor.test(dados$Nota, dados$Ansiedade, method = "kendall")
```
  
Verificamos então que a correlação para todos os casos é diferente de zero.
  
<br>  
  
## Passo 6: Gráfico de dispersão  
  
```{r, warning=FALSE, message=FALSE}
ggplot(dados, aes(x = Ansiedade, y = Nota)) +
  labs(x = "Ansiedade pré-prova", y = "Desempenho na prova") +
  geom_point(size = 1.8) +
  theme_classic()
```
  
<br>
  
## Passo 7 (opcional): Matrizes de correlação
  
### Criando a matriz:  
  
```{r, warning=FALSE, message=FALSE}
matriz <- cor(dados[2:4], method = "pearson")
head.matrix(matriz)
```
  
<br>
  
### Arredondando para duas casas decimais:  
  
```{r, warning=FALSE, message=FALSE}
matriz <- round(cor(dados[2:4], method = "pearson"), 2)
head.matrix(matriz)
```
  
<br>
  
### Criando uma matriz visual (pacote `corrplot`)  
  
```{r, warning=FALSE, message=FALSE}
corrplot(matriz, method = "number")
```
  
Opções de métodos: `method = circle, color, pie`  
Opções de tipos: `type = upper, lower`  
Ordenar: `order = hclust`  
  
```{r, warning=FALSE, message=FALSE}
corrplot(matriz, method = "color", 
         type = "upper", order = "hclust", 
         addCoef.col = "black", # adiciona o coeficiente à matriz
         tl.col = "black", tl.srt = 45, # cor e rotação do nome das variáveis
         diag = FALSE # não mostrar a diagonal principal
         )
```
  
<br>
<br>
  
# Resultado  
  
<p style="text-align: justify;">Verificamos, assim que a correlação de *Spearman* mostrou que há uma correlação negativa e fraca entre a ansidade pré-prova e a nota do aluno (rho = -0,40, p < 0,001).</p>  
  
<br>
<br>